{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Piano MIDI Generation - Model Architecture\n",
        "\n",
        "This notebook defines and builds the transformer model for piano MIDI generation.\n",
        "\n",
        "## Overview\n",
        "\n",
        "1. **Load processed data** - Vocabulary and sequences\n",
        "2. **Define model architecture** - GPT-style decoder transformer\n",
        "3. **Create dataset/dataloader** - For batch training\n",
        "4. **Model initialization** - With proper weights\n",
        "5. **Test forward pass** - Verify everything works\n",
        "\n",
        "## Model Design\n",
        "\n",
        "- **Architecture**: GPT-2 style decoder-only transformer\n",
        "- **Conditional Generation**: Metadata tokens as prefix for MIDI generation\n",
        "- **Autoregressive**: Predicts next token given previous tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support\n",
        "# âš ï¸ IMPORTANT: If you have GPU, install CUDA version (NOT CPU-only!)\n",
        "\n",
        "# Step 1: Uninstall CPU-only PyTorch\n",
        "# %pip uninstall torch torchvision torchaudio -y\n",
        "\n",
        "# Step 2: Check your CUDA version (run in Command Prompt/PowerShell):\n",
        "# nvidia-smi\n",
        "# Look for \"CUDA Version: X.X\" in the output\n",
        "\n",
        "# Step 3: Install matching PyTorch with CUDA (uncomment ONE based on your CUDA version):\n",
        "\n",
        "# For CUDA 11.8:\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# For CUDA 12.1:\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# For CUDA 12.4:\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "# For Windows with CUDA (most common):\n",
        "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# After installing, restart kernel and check CUDA availability in next cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "âœ… CUDA device: NVIDIA GeForce GTX 1650\n",
            "   CUDA version: 12.4\n",
            "   GPU memory: 4.3 GB\n",
            "   Device count: 1\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"   Device count: {torch.cuda.device_count()}\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  WARNING: CUDA not available!\")\n",
        "    print(\"   You have CPU-only PyTorch installed (version shows '+cpu')\")\n",
        "    print(\"\\n   To fix and use your GPU:\")\n",
        "    print(\"   1. Open Command Prompt/PowerShell\")\n",
        "    print(\"   2. Run: nvidia-smi  (to check CUDA version)\")\n",
        "    print(\"   3. Uninstall CPU version:\")\n",
        "    print(\"      pip uninstall torch torchvision torchaudio -y\")\n",
        "    print(\"   4. Install CUDA version (replace cu121 with your CUDA version):\")\n",
        "    print(\"      pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
        "    print(\"   5. Restart Jupyter kernel\")\n",
        "    print(\"\\n   Training will work but will be SLOW on CPU!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Processed Data\n",
        "\n",
        "Load vocabulary and sequences from preprocessing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded vocabulary: 746 tokens\n",
            "âœ… Loaded 3,843 sequences\n",
            "\n",
            "Config:\n",
            "  min_quality_score: 0.97\n",
            "  max_sequence_length: 2048\n",
            "  time_quantization: 10\n",
            "  vocab_size: 746\n",
            "  num_sequences: 3843\n",
            "  total_tokens: 7766992\n"
          ]
        }
      ],
      "source": [
        "# Load processed data\n",
        "data_dir = Path(\"processed_data\")\n",
        "\n",
        "with open(data_dir / \"vocab.json\", 'r') as f:\n",
        "    vocab = json.load(f)\n",
        "\n",
        "with open(data_dir / \"id_to_token.json\", 'r') as f:\n",
        "    id_to_token = json.load(f)\n",
        "\n",
        "with open(data_dir / \"preprocessing_config.json\", 'r') as f:\n",
        "    preprocess_config = json.load(f)\n",
        "\n",
        "with open(data_dir / \"sequences.json\", 'r') as f:\n",
        "    sequences = json.load(f)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"âœ… Loaded vocabulary: {vocab_size:,} tokens\")\n",
        "print(f\"âœ… Loaded {len(sequences):,} sequences\")\n",
        "print(f\"\\nConfig:\")\n",
        "for key, value in preprocess_config.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Convert id_to_token from string keys to int\n",
        "id_to_token = {int(k): v for k, v in id_to_token.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Configuration\n",
        "\n",
        "Define model hyperparameters. **Keep model small to match dataset size.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Configuration:\n",
            "  vocab_size: 746\n",
            "  max_seq_length: 2048\n",
            "  d_model: 512\n",
            "  n_layers: 6\n",
            "  n_heads: 8\n",
            "  d_ff: 2048\n",
            "  dropout: 0.1\n",
            "  pad_token_id: 0\n",
            "\n",
            "ðŸ“Š Approximate model size: 19.3M parameters\n",
            "   Dataset size: 3,843 sequences\n",
            "   Ratio: 199.6 sequences per 1M params\n",
            "   (Good ratio: >100 sequences per 1M params)\n"
          ]
        }
      ],
      "source": [
        "# Model Configuration\n",
        "# Adjusted for small dataset (3,843 sequences)\n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'max_seq_length': preprocess_config['max_sequence_length'],  # 2048\n",
        "    'd_model': 512,        # Embedding dimension (smaller for small dataset)\n",
        "    'n_layers': 6,         # Number of transformer blocks (reduced)\n",
        "    'n_heads': 8,          # Attention heads\n",
        "    'd_ff': 2048,         # Feed-forward dimension\n",
        "    'dropout': 0.1,       # Dropout rate\n",
        "    'pad_token_id': vocab.get('<PAD>', 0),\n",
        "}\n",
        "\n",
        "print(\"Model Configuration:\")\n",
        "for key, value in MODEL_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Calculate approximate parameters\n",
        "# Rough estimate: vocab_size * d_model + n_layers * (d_model^2 * 4)\n",
        "approx_params = (\n",
        "    MODEL_CONFIG['vocab_size'] * MODEL_CONFIG['d_model'] +  # Embedding\n",
        "    MODEL_CONFIG['n_layers'] * (\n",
        "        MODEL_CONFIG['d_model'] ** 2 * 4 +  # Attention\n",
        "        MODEL_CONFIG['d_model'] * MODEL_CONFIG['d_ff'] * 2  # FFN\n",
        "    )\n",
        ")\n",
        "print(f\"\\nðŸ“Š Approximate model size: {approx_params/1e6:.1f}M parameters\")\n",
        "print(f\"   Dataset size: {len(sequences):,} sequences\")\n",
        "print(f\"   Ratio: {len(sequences)/approx_params*1e6:.1f} sequences per 1M params\")\n",
        "print(f\"   (Good ratio: >100 sequences per 1M params)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Transformer Model Architecture\n",
        "\n",
        "GPT-style decoder-only transformer for autoregressive generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Model created with 19.7M parameters\n",
            "\n",
            "ðŸ’¡ Tip: Model classes are also available in 'model.py' for import in other notebooks\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Import from shared module (recommended)\n",
        "# Uncomment this if you've created model.py:\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "from model import PositionalEncoding, TransformerBlock, PianoMIDIGenerator\n",
        "\n",
        "# Option 2: Define classes here (for standalone notebook)\n",
        "# Transformer Architecture Components\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: [batch, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer decoder block\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
        "        # Self-attention with residual\n",
        "        # attn_mask: [seq_len, seq_len] - causal mask (broadcasted by MultiheadAttention)\n",
        "        # key_padding_mask: [batch, seq_len] - True = pad (mask), False = valid\n",
        "        \n",
        "        # MultiheadAttention handles:\n",
        "        # - attn_mask: 2D [seq_len, seq_len] gets broadcasted to [batch, num_heads, seq_len, seq_len]\n",
        "        # - key_padding_mask: [batch, seq_len] - masks out padded positions\n",
        "        \n",
        "        attn_out, _ = self.attention(\n",
        "            x, x, x, \n",
        "            attn_mask=attn_mask,  # Causal mask: [seq_len, seq_len]\n",
        "            key_padding_mask=key_padding_mask  # Padding mask: [batch, seq_len] or None\n",
        "        )\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        \n",
        "        # Feed-forward with residual\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class PianoMIDIGenerator(nn.Module):\n",
        "    \"\"\"\n",
        "    GPT-style decoder-only transformer for conditional MIDI generation\n",
        "    \n",
        "    Input: [metadata_tokens] + [midi_tokens]\n",
        "    Output: Next token predictions for all positions\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.d_model = config['d_model']\n",
        "        \n",
        "        # Token embeddings\n",
        "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        \n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(config['d_model'], config['max_seq_length'])\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                config['d_model'],\n",
        "                config['n_heads'],\n",
        "                config['d_ff'],\n",
        "                config['dropout']\n",
        "            )\n",
        "            for _ in range(config['n_layers'])\n",
        "        ])\n",
        "        \n",
        "        # Output projection\n",
        "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
        "        self.head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: [batch, seq_len] token IDs\n",
        "            attention_mask: [batch, seq_len] 1 = attend, 0 = mask\n",
        "        \n",
        "        Returns:\n",
        "            logits: [batch, seq_len, vocab_size]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # Embeddings\n",
        "        x = self.embedding(input_ids)  # [batch, seq_len, d_model]\n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        # Create causal mask (lower triangular) for autoregressive generation\n",
        "        # MultiheadAttention with batch_first=True expects:\n",
        "        # - 2D mask: [seq_len, seq_len] (broadcasted to batch and heads)\n",
        "        # - 3D mask: NOT directly supported in same way\n",
        "        \n",
        "        # Base causal mask: lower triangular [seq_len, seq_len]\n",
        "        causal_mask_base = torch.tril(torch.ones(seq_len, seq_len, device=input_ids.device)).bool()\n",
        "        \n",
        "        # If attention_mask provided, we need to handle padding\n",
        "        # For MultiheadAttention, we'll create a combined mask\n",
        "        if attention_mask is not None:\n",
        "            # attention_mask: [batch, seq_len] where 1 = valid, 0 = pad\n",
        "            # Create 2D mask per batch, then combine\n",
        "            # We'll use key_padding_mask instead for padding (better for MultiheadAttention)\n",
        "            key_padding_mask = (attention_mask == 0)  # True = pad (mask), False = valid\n",
        "            \n",
        "            # For causal mask, use base mask (will be combined with key_padding_mask internally)\n",
        "            # But MultiheadAttention doesn't directly support causal + padding mask together\n",
        "            # So we'll create combined mask manually\n",
        "            causal_mask_2d = causal_mask_base  # [seq_len, seq_len]\n",
        "            # This will be broadcasted to all batches and heads by MultiheadAttention\n",
        "            attn_mask = causal_mask_2d\n",
        "        else:\n",
        "            # Just causal mask, no padding\n",
        "            attn_mask = causal_mask_base  # [seq_len, seq_len]\n",
        "            key_padding_mask = None\n",
        "        \n",
        "        # Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
        "        \n",
        "        # Output projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # [batch, seq_len, vocab_size]\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def generate(self, input_ids, max_length=2048, temperature=1.0, top_k=50, top_p=0.9):\n",
        "        \"\"\"\n",
        "        Autoregressive generation\n",
        "        \n",
        "        Args:\n",
        "            input_ids: [batch, start_len] initial tokens (metadata)\n",
        "            max_length: Maximum generation length\n",
        "            temperature: Sampling temperature\n",
        "            top_k: Top-k sampling\n",
        "            top_p: Nucleus sampling\n",
        "        \n",
        "        Returns:\n",
        "            Generated token IDs\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        generated = input_ids.clone()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length - input_ids.size(1)):\n",
        "                # Forward pass\n",
        "                logits = self.forward(generated)[:, -1, :]  # [batch, vocab_size]\n",
        "                \n",
        "                # Apply temperature\n",
        "                logits = logits / temperature\n",
        "                \n",
        "                # Top-k filtering\n",
        "                if top_k > 0:\n",
        "                    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "                    logits[indices_to_remove] = float('-inf')\n",
        "                \n",
        "                # Top-p (nucleus) sampling\n",
        "                if top_p < 1.0:\n",
        "                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                    \n",
        "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                    sorted_indices_to_remove[..., 0] = 0\n",
        "                    \n",
        "                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                    logits[indices_to_remove] = float('-inf')\n",
        "                \n",
        "                # Sample next token\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "                generated = torch.cat([generated, next_token], dim=1)\n",
        "                \n",
        "                # Stop if END token\n",
        "                if next_token.item() == vocab.get('<END>', 3):\n",
        "                    break\n",
        "        \n",
        "        return generated\n",
        "\n",
        "# Create model\n",
        "model = PianoMIDIGenerator(MODEL_CONFIG)\n",
        "print(f\"âœ… Model created with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")\n",
        "\n",
        "# Save model classes to Python file for reuse in training notebook\n",
        "# This allows training notebook to import without duplicating code\n",
        "model_code = '''\n",
        "# Model classes saved from architecture notebook\n",
        "# These are also in model.py - use that for cleaner imports\n",
        "'''\n",
        "print(\"\\nðŸ’¡ Tip: Model classes are also available in 'model.py' for import in other notebooks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Dataset created:\n",
            "  Total sequences: 3,843\n",
            "  Train: 3,458\n",
            "  Validation: 385\n",
            "  Batch size: 8\n",
            "  Train batches: 433\n",
            "  Val batches: 49\n"
          ]
        }
      ],
      "source": [
        "# Dataset class\n",
        "class MIDIDataset(Dataset):\n",
        "    \"\"\"Dataset for MIDI token sequences\"\"\"\n",
        "    def __init__(self, sequences, vocab, max_length=2048):\n",
        "        self.sequences = sequences\n",
        "        self.vocab = vocab\n",
        "        self.pad_token_id = vocab.get('<PAD>', 0)\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = self.sequences[idx]['token_ids']\n",
        "        \n",
        "        # Truncate if needed\n",
        "        if len(token_ids) > self.max_length:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "        \n",
        "        # Create input (all tokens except last) and target (all tokens except first)\n",
        "        input_ids = token_ids[:-1]\n",
        "        target_ids = token_ids[1:]\n",
        "        \n",
        "        # Pad to max_length\n",
        "        pad_len = self.max_length - len(input_ids)\n",
        "        if pad_len > 0:\n",
        "            input_ids = input_ids + [self.pad_token_id] * pad_len\n",
        "            target_ids = target_ids + [self.pad_token_id] * pad_len\n",
        "        \n",
        "        # Create attention mask (1 for real tokens, 0 for padding)\n",
        "        attention_mask = [1] * len(token_ids[:-1]) + [0] * pad_len\n",
        "        \n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = MIDIDataset(sequences, vocab, max_length=MODEL_CONFIG['max_seq_length'])\n",
        "\n",
        "# Split into train/val (90/10)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, \n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# DataLoaders\n",
        "BATCH_SIZE = 8  # Small batch size for small dataset\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Set to 0 for Windows/Jupyter\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"âœ… Dataset created:\")\n",
        "print(f\"  Total sequences: {len(dataset):,}\")\n",
        "print(f\"  Train: {len(train_dataset):,}\")\n",
        "print(f\"  Validation: {len(val_dataset):,}\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Train batches: {len(train_loader):,}\")\n",
        "print(f\"  Val batches: {len(val_loader):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Test Forward Pass\n",
        "\n",
        "Verify the model works correctly with a sample batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample batch shape:\n",
            "  input_ids: torch.Size([8, 2048])\n",
            "  attention_mask: torch.Size([8, 2048])\n",
            "  target_ids: torch.Size([8, 2048])\n",
            "\n",
            "âœ… Forward pass successful!\n",
            "  Logits shape: torch.Size([8, 2048, 746])\n",
            "  Expected: [8, 2048, 746]\n",
            "\n",
            "âœ… Loss computation successful!\n",
            "  Loss: nan\n",
            "\n",
            "Sample predictions (first 20 tokens):\n",
            "  Actual:   ['TIME_SHIFT:120', 'NOTE_ON:44', 'VELOCITY:80', 'TIME_SHIFT:80', 'NOTE_ON:85', 'VELOCITY:75', 'NOTE_ON:73', 'VELOCITY:80', 'TIME_SHIFT:210', 'NOTE_ON:56', 'VELOCITY:80', 'NOTE_ON:72', 'VELOCITY:90', 'TIME_SHIFT:10', 'NOTE_ON:80', 'VELOCITY:80', 'NOTE_ON:84', 'VELOCITY:85', 'TIME_SHIFT:70', 'NOTE_ON:70']\n",
            "  Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "\n",
            "âœ… Model architecture verified! Ready for training.\n"
          ]
        }
      ],
      "source": [
        "# Test forward pass\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Get a sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "input_ids = sample_batch['input_ids'].to(device)\n",
        "attention_mask = sample_batch['attention_mask'].to(device)\n",
        "target_ids = sample_batch['target_ids'].to(device)\n",
        "\n",
        "print(f\"Sample batch shape:\")\n",
        "print(f\"  input_ids: {input_ids.shape}\")\n",
        "print(f\"  attention_mask: {attention_mask.shape}\")\n",
        "print(f\"  target_ids: {target_ids.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    logits = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "print(f\"\\nâœ… Forward pass successful!\")\n",
        "print(f\"  Logits shape: {logits.shape}\")  # Should be [batch, seq_len, vocab_size]\n",
        "print(f\"  Expected: [{input_ids.shape[0]}, {input_ids.shape[1]}, {vocab_size}]\")\n",
        "\n",
        "# Test loss computation\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=MODEL_CONFIG['pad_token_id'])\n",
        "logits_flat = logits.view(-1, vocab_size)\n",
        "targets_flat = target_ids.view(-1)\n",
        "loss = criterion(logits_flat, targets_flat)\n",
        "\n",
        "print(f\"\\nâœ… Loss computation successful!\")\n",
        "print(f\"  Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Show sample predictions\n",
        "predicted_tokens = torch.argmax(logits[0, :20, :], dim=-1)\n",
        "actual_tokens = target_ids[0, :20]\n",
        "\n",
        "print(f\"\\nSample predictions (first 20 tokens):\")\n",
        "print(f\"  Actual:   {[id_to_token[t.item()] for t in actual_tokens]}\")\n",
        "print(f\"  Predicted: {[id_to_token[t.item()] for t in predicted_tokens]}\")\n",
        "\n",
        "print(f\"\\nâœ… Model architecture verified! Ready for training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
