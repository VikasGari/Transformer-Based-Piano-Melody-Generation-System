{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Piano MIDI Generation - Complete Pipeline (Kaggle)\n",
        "\n",
        "This notebook contains the complete pipeline for training a Transformer model on the ARIA MIDI dataset:\n",
        "\n",
        "1. **Preprocessing** - Metadata analysis, tokenization, dataset creation\n",
        "2. **Model Architecture** - GPT-style decoder-only transformer\n",
        "3. **Training** - Full training loop with validation and checkpointing\n",
        "\n",
        "## Optimized for Kaggle P100 GPU\n",
        "- Larger batch sizes for better GPU utilization\n",
        "- Memory-efficient data loading\n",
        "- Full preprocessing + training pipeline\n",
        "\n",
        "## Dataset Location\n",
        "- Input: `/kaggle/input/aria-midi-v1-deduped-ext`\n",
        "- Output: `/kaggle/working/` (checkpoints, processed data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'c:\\Users\\Vikas' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (Kaggle has most packages, but just in case)\n",
        "import sys\n",
        "!{sys.executable} -m pip install mido pretty_midi tqdm -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import mido\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"‚úÖ All libraries imported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle paths\n",
        "INPUT_DIR = Path(\"/kaggle/input/aria-midi-v1-deduped-ext\")\n",
        "WORKING_DIR = Path(\"/kaggle/working\")\n",
        "\n",
        "# Create working directories\n",
        "WORKING_DIR.mkdir(exist_ok=True)\n",
        "(WORKING_DIR / \"processed_data\").mkdir(exist_ok=True)\n",
        "(WORKING_DIR / \"checkpoints\").mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úÖ Input directory: {INPUT_DIR}\")\n",
        "print(f\"‚úÖ Working directory: {WORKING_DIR}\")\n",
        "print(f\"‚úÖ Dataset exists: {INPUT_DIR.exists()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"‚úÖ Using GPU: {gpu_name}\")\n",
        "    print(f\"   Total memory: {gpu_memory:.2f} GB\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"‚ö†Ô∏è  Using CPU (CUDA not available)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Model Architecture\n",
        "\n",
        "Define the Transformer model classes inline (from model.py)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Architecture (from model.py)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer decoder block\"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
        "        # Check for NaN/Inf\n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        attn_out, _ = self.attention(x, x, x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
        "        \n",
        "        if torch.isnan(attn_out).any() or torch.isinf(attn_out).any():\n",
        "            attn_out = torch.nan_to_num(attn_out, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "        \n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        ff_out = self.ff(x)\n",
        "        \n",
        "        if torch.isnan(ff_out).any() or torch.isinf(ff_out).any():\n",
        "            ff_out = torch.nan_to_num(ff_out, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        x = self.norm2(x + ff_out)\n",
        "        \n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "class PianoMIDIGenerator(nn.Module):\n",
        "    \"\"\"GPT-style decoder-only transformer for conditional MIDI generation\"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.d_model = config['d_model']\n",
        "        \n",
        "        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.pos_encoding = PositionalEncoding(config['d_model'], config['max_seq_length'])\n",
        "        \n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(config['d_model'], config['n_heads'], config['d_ff'], config['dropout'])\n",
        "            for _ in range(config['n_layers'])\n",
        "        ])\n",
        "        \n",
        "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
        "        self.head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)\n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight, gain=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.01)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        x = self.embedding(input_ids)\n",
        "        \n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        # Causal mask: True = masked (don't attend)\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1).bool()\n",
        "        \n",
        "        if attention_mask is not None:\n",
        "            key_padding_mask = (attention_mask == 0)\n",
        "            attn_mask = causal_mask\n",
        "        else:\n",
        "            attn_mask = causal_mask\n",
        "            key_padding_mask = None\n",
        "        \n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x, attn_mask=attn_mask, key_padding_mask=key_padding_mask)\n",
        "            if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "                x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        x = self.ln_f(x)\n",
        "        \n",
        "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
        "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "        \n",
        "        logits = self.head(x)\n",
        "        \n",
        "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "            logits = torch.clamp(logits, min=-50.0, max=50.0)\n",
        "            logits = torch.nan_to_num(logits, nan=0.0, posinf=50.0, neginf=-50.0)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "print(\"‚úÖ Model architecture defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "metadata_path = INPUT_DIR / \"metadata.json\"\n",
        "\n",
        "print(f\"Loading metadata from: {metadata_path}\")\n",
        "with open(metadata_path, 'r', encoding='utf-8') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(metadata):,} entries\")\n",
        "print(f\"Sample entry keys: {list(metadata.keys())[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metadata tokenizer\n",
        "class MetadataTokenizer:\n",
        "    def __init__(self, include_composer=True, top_n_composers=100):\n",
        "        self.include_composer = include_composer\n",
        "        self.valid_genres = {'classical', 'pop', 'soundtrack', 'jazz', 'rock', 'folk', 'ambient', 'ragtime', 'blues', 'atonal'}\n",
        "        self.valid_periods = {'contemporary', 'modern', 'romantic', 'classical', 'baroque', 'impressionist'}\n",
        "        self.top_composers = self._load_top_composers(top_n_composers)\n",
        "    \n",
        "    def _load_top_composers(self, n):\n",
        "        top = {'hisaishi', 'satie', 'yiruma', 'einaudi', 'joplin', 'chopin', 'beethoven', 'bach', 'mozart', 'debussy',\n",
        "               'schubert', 'schumann', 'liszt', 'rachmaninoff', 'tchaikovsky', 'ravel', 'poulenc', 'faure', 'bartok'}\n",
        "        return {self._normalize_composer(c) for c in top}\n",
        "    \n",
        "    def _normalize_composer(self, composer):\n",
        "        if not composer:\n",
        "            return \"\"\n",
        "        normalized = composer.lower().strip()\n",
        "        normalized = normalized.replace('√©', 'e').replace('√®', 'e').replace('√°', 'a').replace('√†', 'a')\n",
        "        normalized = normalized.replace('√≠', 'i').replace('√¨', 'i').replace('√≥', 'o').replace('√≤', 'o')\n",
        "        normalized = normalized.replace('√∫', 'u').replace('√π', 'u').replace('√±', 'n')\n",
        "        normalized = re.sub(r'[^a-z0-9\\s-]', '', normalized)\n",
        "        normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
        "        return normalized\n",
        "    \n",
        "    def metadata_to_tokens(self, metadata, include_start=True):\n",
        "        tokens = []\n",
        "        if include_start:\n",
        "            tokens.append(\"START\")\n",
        "        \n",
        "        if metadata.get('genre'):\n",
        "            genre = metadata['genre'].lower().strip()\n",
        "            if genre in self.valid_genres:\n",
        "                tokens.append(f\"GENRE:{genre}\")\n",
        "        \n",
        "        if metadata.get('music_period'):\n",
        "            period = metadata['music_period'].lower().strip()\n",
        "            if period in self.valid_periods:\n",
        "                tokens.append(f\"PERIOD:{period}\")\n",
        "        \n",
        "        if self.include_composer and metadata.get('composer'):\n",
        "            composer = self._normalize_composer(metadata['composer'])\n",
        "            if composer in self.top_composers:\n",
        "                tokens.append(f\"COMPOSER:{composer}\")\n",
        "        \n",
        "        return tokens\n",
        "\n",
        "meta_tokenizer = MetadataTokenizer(include_composer=True)\n",
        "print(\"‚úÖ Metadata tokenizer created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MIDI tokenizer\n",
        "class MIDITokenizer:\n",
        "    def __init__(self, time_quantization=10):\n",
        "        self.time_quantization = time_quantization\n",
        "    \n",
        "    def midi_to_tokens(self, midi_path: Path) -> List[str]:\n",
        "        try:\n",
        "            mid = mido.MidiFile(midi_path)\n",
        "            tokens = []\n",
        "            current_time = 0\n",
        "            \n",
        "            for track in mid.tracks:\n",
        "                for msg in track:\n",
        "                    current_time += int(msg.time)\n",
        "                    quantized_time = (current_time // self.time_quantization) * self.time_quantization\n",
        "                    \n",
        "                    if msg.type == 'note_on' and msg.velocity > 0:\n",
        "                        if quantized_time > 0:\n",
        "                            tokens.append(f\"TIME_SHIFT:{quantized_time}\")\n",
        "                        tokens.append(f\"NOTE_ON:{msg.note}\")\n",
        "                        tokens.append(f\"VELOCITY:{msg.velocity}\")\n",
        "                        current_time = 0\n",
        "                    elif msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):\n",
        "                        if quantized_time > 0:\n",
        "                            tokens.append(f\"TIME_SHIFT:{quantized_time}\")\n",
        "                        tokens.append(f\"NOTE_OFF:{msg.note}\")\n",
        "                        current_time = 0\n",
        "            \n",
        "            return tokens\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "midi_tokenizer = MIDITokenizer(time_quantization=10)\n",
        "print(\"‚úÖ MIDI tokenizer created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find MIDI file helper\n",
        "def find_midi_file(file_id: str, audio_index: str, data_root: Path) -> Optional[Path]:\n",
        "    padded_id = file_id.zfill(6)\n",
        "    filename = f\"{padded_id}_{audio_index}.mid\"\n",
        "    \n",
        "    for subfolder in data_root.iterdir():\n",
        "        if subfolder.is_dir() and len(subfolder.name) == 2:\n",
        "            filepath = subfolder / filename\n",
        "            if filepath.exists():\n",
        "                return filepath\n",
        "    return None\n",
        "\n",
        "print(\"‚úÖ File finder helper defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Dataset Processing - Balanced Sampling and Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing configuration\n",
        "PREPROCESS_CONFIG = {\n",
        "    'min_quality_score': 0.97,\n",
        "    'max_sequence_length': 2048,\n",
        "    'time_quantization': 10,\n",
        "    'data_root': INPUT_DIR / \"data\",\n",
        "}\n",
        "\n",
        "# Balanced sampling config\n",
        "SAMPLING_CONFIG = {\n",
        "    'max_per_composer': 500,\n",
        "    'max_empty_metadata_ratio': 0.05,\n",
        "    'composer_strategy': 'balanced',\n",
        "}\n",
        "\n",
        "def analyze_metadata_distribution(metadata_dict):\n",
        "    stats = {\n",
        "        'by_composer': defaultdict(int),\n",
        "        'by_genre': defaultdict(int),\n",
        "        'empty_metadata': [],\n",
        "        'with_composer': [],\n",
        "        'no_composer': [],\n",
        "    }\n",
        "    \n",
        "    for entry_id, entry_data in metadata_dict.items():\n",
        "        metadata = entry_data.get('metadata', {})\n",
        "        audio_scores = entry_data.get('audio_scores', {})\n",
        "        \n",
        "        if not audio_scores:\n",
        "            continue\n",
        "        best_score = max(audio_scores.values())\n",
        "        if best_score < PREPROCESS_CONFIG['min_quality_score']:\n",
        "            continue\n",
        "        \n",
        "        genre = metadata.get('genre', '').lower() if metadata.get('genre') else None\n",
        "        composer = metadata.get('composer', '').lower() if metadata.get('composer') else None\n",
        "        \n",
        "        if not metadata:\n",
        "            stats['empty_metadata'].append(entry_id)\n",
        "        else:\n",
        "            if genre:\n",
        "                stats['by_genre'][genre] += 1\n",
        "            if composer:\n",
        "                stats['by_composer'][composer] += 1\n",
        "                stats['with_composer'].append(entry_id)\n",
        "            else:\n",
        "                stats['no_composer'].append(entry_id)\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def create_balanced_sample(metadata_dict, sampling_config, tokenizer):\n",
        "    random.seed(42)\n",
        "    distribution = analyze_metadata_distribution(metadata_dict)\n",
        "    \n",
        "    balanced_ids = []\n",
        "    composer_samples = defaultdict(int)\n",
        "    \n",
        "    # Sample top composers\n",
        "    top_composers = sorted(distribution['by_composer'].items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "    \n",
        "    for composer, _ in top_composers:\n",
        "        normalized = tokenizer._normalize_composer(composer)\n",
        "        if normalized in tokenizer.top_composers:\n",
        "            composer_ids = [eid for eid in distribution['with_composer'] \n",
        "                          if tokenizer._normalize_composer(metadata_dict[eid].get('metadata', {}).get('composer', '')) == normalized]\n",
        "            random.shuffle(composer_ids)\n",
        "            sample_count = min(sampling_config['max_per_composer'], len(composer_ids))\n",
        "            balanced_ids.extend(composer_ids[:sample_count])\n",
        "    \n",
        "    # Add no-composer samples\n",
        "    random.shuffle(distribution['no_composer'])\n",
        "    no_composer_count = len(balanced_ids)  # Match composer count\n",
        "    balanced_ids.extend(distribution['no_composer'][:no_composer_count])\n",
        "    \n",
        "    random.shuffle(balanced_ids)\n",
        "    return balanced_ids\n",
        "\n",
        "print(\"‚úÖ Preprocessing functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process full dataset\n",
        "def process_dataset(metadata_dict, data_root, balanced_ids=None):\n",
        "    sequences = []\n",
        "    \n",
        "    entries_to_process = balanced_ids if balanced_ids else list(metadata_dict.keys())\n",
        "    \n",
        "    for entry_id in tqdm(entries_to_process, desc=\"Processing entries\"):\n",
        "        entry_data = metadata_dict.get(entry_id)\n",
        "        if not entry_data:\n",
        "            continue\n",
        "        \n",
        "        audio_scores = entry_data.get('audio_scores', {})\n",
        "        if not audio_scores:\n",
        "            continue\n",
        "        \n",
        "        best_idx = max(audio_scores.items(), key=lambda x: x[1])[0]\n",
        "        score = audio_scores[best_idx]\n",
        "        \n",
        "        if score < PREPROCESS_CONFIG['min_quality_score']:\n",
        "            continue\n",
        "        \n",
        "        metadata_dict_entry = entry_data.get('metadata', {})\n",
        "        metadata_tokens = meta_tokenizer.metadata_to_tokens(metadata_dict_entry, include_start=True)\n",
        "        \n",
        "        midi_path = find_midi_file(entry_id, best_idx, data_root)\n",
        "        if not midi_path or not midi_path.exists():\n",
        "            continue\n",
        "        \n",
        "        midi_tokens = midi_tokenizer.midi_to_tokens(midi_path)\n",
        "        if not midi_tokens:\n",
        "            continue\n",
        "        \n",
        "        full_sequence = metadata_tokens + midi_tokens + [\"<END>\"]\n",
        "        \n",
        "        if len(full_sequence) > PREPROCESS_CONFIG['max_sequence_length']:\n",
        "            metadata_len = len(metadata_tokens)\n",
        "            max_midi_len = PREPROCESS_CONFIG['max_sequence_length'] - metadata_len - 1\n",
        "            full_sequence = metadata_tokens + midi_tokens[:max_midi_len] + [\"<END>\"]\n",
        "        \n",
        "        sequences.append({\n",
        "            'entry_id': entry_id,\n",
        "            'tokens': full_sequence\n",
        "        })\n",
        "    \n",
        "    return sequences\n",
        "\n",
        "# Check if processed data already exists\n",
        "output_dir = WORKING_DIR / \"processed_data\"\n",
        "sequences_file = output_dir / \"sequences.json\"\n",
        "vocab_file = output_dir / \"vocab.json\"\n",
        "id_to_token_file = output_dir / \"id_to_token.json\"\n",
        "\n",
        "if sequences_file.exists() and vocab_file.exists() and id_to_token_file.exists():\n",
        "    print(\"‚úÖ Found existing processed data - loading from files...\")\n",
        "    print(f\"   Loading from: {output_dir}\")\n",
        "    \n",
        "    # Load sequences\n",
        "    with open(sequences_file, 'r') as f:\n",
        "        sequences_data = json.load(f)\n",
        "    \n",
        "    # Convert loaded sequences back to format with 'tokens' (we'll rebuild from token_ids)\n",
        "    all_sequences = sequences_data\n",
        "    \n",
        "    print(f\"‚úÖ Loaded {len(all_sequences):,} sequences from existing files\")\n",
        "    print(\"‚ö†Ô∏è  Note: Will rebuild vocabulary and token IDs in next step\")\n",
        "else:\n",
        "    print(\"üìù No existing processed data found - starting preprocessing...\")\n",
        "    \n",
        "    # Create balanced sample\n",
        "    print(\"Creating balanced dataset sample...\")\n",
        "    balanced_ids = create_balanced_sample(metadata, SAMPLING_CONFIG, meta_tokenizer)\n",
        "    balanced_metadata = {eid: metadata[eid] for eid in balanced_ids if eid in metadata}\n",
        "    \n",
        "    print(f\"‚úÖ Balanced sample: {len(balanced_metadata):,} entries\")\n",
        "    \n",
        "    # Process dataset\n",
        "    print(\"\\nProcessing MIDI files...\")\n",
        "    all_sequences = process_dataset(balanced_metadata, PREPROCESS_CONFIG['data_root'], balanced_ids)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Processed {len(all_sequences):,} sequences\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build vocabulary or load existing\n",
        "output_dir = WORKING_DIR / \"processed_data\"\n",
        "vocab_file = output_dir / \"vocab.json\"\n",
        "id_to_token_file = output_dir / \"id_to_token.json\"\n",
        "sequences_file = output_dir / \"sequences.json\"\n",
        "\n",
        "if vocab_file.exists() and id_to_token_file.exists() and sequences_file.exists():\n",
        "    print(\"‚úÖ Loading existing vocabulary and sequences...\")\n",
        "    \n",
        "    # Load vocabulary\n",
        "    with open(vocab_file, 'r') as f:\n",
        "        vocab = json.load(f)\n",
        "    \n",
        "    # Load id_to_token\n",
        "    with open(id_to_token_file, 'r') as f:\n",
        "        id_to_token = json.load(f)\n",
        "    \n",
        "    # Load sequences (should already have token_ids)\n",
        "    with open(sequences_file, 'r') as f:\n",
        "        all_sequences = json.load(f)\n",
        "    \n",
        "    # Ensure vocab values are ints and id_to_token keys are ints\n",
        "    vocab = {str(k): int(v) if isinstance(v, str) else int(v) for k, v in vocab.items()}\n",
        "    id_to_token = {int(k): str(v) if not isinstance(v, str) else v for k, v in id_to_token.items()}\n",
        "    \n",
        "    # Ensure all_sequences have token_ids (in case they don't)\n",
        "    for seq in all_sequences:\n",
        "        if 'token_ids' not in seq or not seq['token_ids']:\n",
        "            # This shouldn't happen if files were saved correctly, but just in case\n",
        "            print(f\"‚ö†Ô∏è  Warning: Sequence {seq.get('entry_id', 'unknown')} missing token_ids\")\n",
        "    \n",
        "    vocab_size = len(vocab)\n",
        "    pad_token_id = vocab.get('<PAD>', 0)\n",
        "    \n",
        "    print(f\"‚úÖ Loaded vocabulary: {vocab_size:,} tokens\")\n",
        "    print(f\"‚úÖ Loaded sequences: {len(all_sequences):,}\")\n",
        "    print(f\"‚úÖ Data loaded from: {output_dir}\")\n",
        "else:\n",
        "    print(\"üìù Building vocabulary from processed sequences...\")\n",
        "    \n",
        "    # Build vocabulary from tokens\n",
        "    all_tokens = set()\n",
        "    for seq in all_sequences:\n",
        "        all_tokens.update(seq['tokens'])\n",
        "    \n",
        "    vocab = {\n",
        "        '<PAD>': 0,\n",
        "        '<UNK>': 1,\n",
        "        '<START>': 2,\n",
        "        '<END>': 3,\n",
        "    }\n",
        "    \n",
        "    for token in sorted(all_tokens):\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)\n",
        "    \n",
        "    # Convert sequences to token IDs\n",
        "    for seq in all_sequences:\n",
        "        seq['token_ids'] = [vocab.get(token, vocab['<UNK>']) for token in seq['tokens']]\n",
        "    \n",
        "    id_to_token = {v: k for k, v in vocab.items()}\n",
        "    vocab_size = len(vocab)\n",
        "    pad_token_id = vocab['<PAD>']\n",
        "    \n",
        "    print(f\"‚úÖ Vocabulary size: {vocab_size:,}\")\n",
        "    print(f\"‚úÖ Total sequences: {len(all_sequences):,}\")\n",
        "    \n",
        "    # Save processed data\n",
        "    output_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    with open(output_dir / \"vocab.json\", 'w') as f:\n",
        "        json.dump(vocab, f)\n",
        "    \n",
        "    with open(output_dir / \"id_to_token.json\", 'w') as f:\n",
        "        json.dump(id_to_token, f)\n",
        "    \n",
        "    sequences_to_save = [{'entry_id': s['entry_id'], 'token_ids': s['token_ids']} for s in all_sequences]\n",
        "    with open(output_dir / \"sequences.json\", 'w') as f:\n",
        "        json.dump(sequences_to_save, f)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Saved processed data to: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Model Configuration and Dataset Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration (optimized for P100)\n",
        "MODEL_CONFIG = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'd_model': 512,\n",
        "    'n_heads': 8,\n",
        "    'n_layers': 6,\n",
        "    'd_ff': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'max_seq_length': 2048,\n",
        "}\n",
        "\n",
        "# Training configuration (optimized for P100 - memory efficient)\n",
        "TRAIN_CONFIG = {\n",
        "    'num_epochs': 50,\n",
        "    'learning_rate': 6e-5,\n",
        "    'weight_decay': 0.1,\n",
        "    'warmup_steps': 500,\n",
        "    'max_grad_norm': 1.0,\n",
        "    'batch_size': 4,  # Reduced for 16GB GPU memory\n",
        "    'accumulation_steps': 4,  # Effective batch size: 16\n",
        "    'eval_steps': 100,\n",
        "    'save_steps': 500,\n",
        "    'patience': 5,\n",
        "    'checkpoint_dir': WORKING_DIR / \"checkpoints\",\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Model and training config set\")\n",
        "print(f\"   Model params: {MODEL_CONFIG['d_model']}d_model, {MODEL_CONFIG['n_layers']} layers, {MODEL_CONFIG['n_heads']} heads\")\n",
        "print(f\"   Training: batch_size={TRAIN_CONFIG['batch_size']}, effective={TRAIN_CONFIG['batch_size'] * TRAIN_CONFIG['accumulation_steps']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class MIDIDataset(Dataset):\n",
        "    def __init__(self, sequences, vocab, max_length=2048):\n",
        "        self.sequences = sequences\n",
        "        self.vocab = vocab\n",
        "        self.pad_token_id = vocab.get('<PAD>', 0)\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = self.sequences[idx]['token_ids']\n",
        "        if len(token_ids) > self.max_length:\n",
        "            token_ids = token_ids[:self.max_length]\n",
        "        input_ids = token_ids[:-1]\n",
        "        target_ids = token_ids[1:]\n",
        "        pad_len = self.max_length - len(input_ids)\n",
        "        if pad_len > 0:\n",
        "            input_ids = input_ids + [self.pad_token_id] * pad_len\n",
        "            target_ids = target_ids + [self.pad_token_id] * pad_len\n",
        "        attention_mask = [1] * len(token_ids[:-1]) + [0] * pad_len\n",
        "        return {\n",
        "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
        "            'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Create dataset and dataloaders\n",
        "dataset = MIDIDataset(all_sequences, vocab, max_length=MODEL_CONFIG['max_seq_length'])\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=TRAIN_CONFIG['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=0,  # Reduced to 0 to save memory\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=TRAIN_CONFIG['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=0,  # Reduced to 0 to save memory\n",
        "    pin_memory=True,\n",
        "    persistent_workers=False\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset: Train={len(train_dataset):,}, Val={len(val_dataset):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear cache before model creation\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Initialize model\n",
        "model = PianoMIDIGenerator(MODEL_CONFIG)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"‚úÖ Model created: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")\n",
        "print(f\"‚úÖ Model on device: {next(model.parameters()).device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()  # Clear cache after model creation\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    print(f\"‚úÖ GPU memory (model loaded):\")\n",
        "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"   Reserved: {reserved:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup optimizer, scheduler, loss\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=TRAIN_CONFIG['learning_rate'],\n",
        "    weight_decay=TRAIN_CONFIG['weight_decay'],\n",
        "    betas=(0.9, 0.95)\n",
        ")\n",
        "\n",
        "def get_lr_scheduler(optimizer, num_training_steps, warmup_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        progress = float(current_step - warmup_steps) / float(max(1, num_training_steps - warmup_steps))\n",
        "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "total_steps = len(train_loader) * TRAIN_CONFIG['num_epochs'] // TRAIN_CONFIG['accumulation_steps']\n",
        "scheduler = get_lr_scheduler(optimizer, total_steps, TRAIN_CONFIG['warmup_steps'])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "\n",
        "print(f\"‚úÖ Training setup complete\")\n",
        "print(f\"   Optimizer: AdamW\")\n",
        "print(f\"   Scheduler: Cosine with {TRAIN_CONFIG['warmup_steps']} step warmup\")\n",
        "print(f\"   Total training steps: {total_steps:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Training Loop with Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training functions\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "learning_rates = []\n",
        "best_val_loss = float('inf')\n",
        "steps_without_improvement = 0\n",
        "\n",
        "def train_step(model, batch, optimizer, criterion, accumulation_steps, global_step):\n",
        "    model.train()\n",
        "    \n",
        "    input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "    attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "    target_ids = batch['target_ids'].to(device, non_blocking=True)\n",
        "    \n",
        "    if torch.isnan(input_ids).any() or torch.isnan(target_ids).any():\n",
        "        return float('nan')\n",
        "    \n",
        "    logits = model(input_ids, attention_mask=attention_mask)\n",
        "    \n",
        "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "        logits = torch.nan_to_num(logits, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "    \n",
        "    logits_flat = logits.view(-1, vocab_size)\n",
        "    targets_flat = target_ids.view(-1)\n",
        "    loss = criterion(logits_flat, targets_flat)\n",
        "    \n",
        "    if torch.isnan(loss) or torch.isinf(loss):\n",
        "        return float('nan')\n",
        "    \n",
        "    loss = loss / accumulation_steps\n",
        "    loss.backward()\n",
        "    \n",
        "    for param in model.parameters():\n",
        "        if param.grad is not None:\n",
        "            if torch.isnan(param.grad).any() or torch.isinf(param.grad).any():\n",
        "                param.grad = torch.nan_to_num(param.grad, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "    \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), TRAIN_CONFIG['max_grad_norm'])\n",
        "    \n",
        "    if (global_step + 1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()  # Clear cache after optimizer step\n",
        "            torch.cuda.synchronize()\n",
        "    \n",
        "    return loss.item() * accumulation_steps\n",
        "\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
        "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
        "            target_ids = batch['target_ids'].to(device, non_blocking=True)\n",
        "            \n",
        "            logits = model(input_ids, attention_mask=attention_mask)\n",
        "            \n",
        "            if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "                logits = torch.nan_to_num(logits, nan=0.0, posinf=10.0, neginf=-10.0)\n",
        "            \n",
        "            logits_flat = logits.view(-1, vocab_size)\n",
        "            targets_flat = target_ids.view(-1)\n",
        "            mask = (targets_flat != pad_token_id)\n",
        "            \n",
        "            if mask.sum() > 0:\n",
        "                loss = criterion(logits_flat, targets_flat)\n",
        "                if not (torch.isnan(loss) or torch.isinf(loss)):\n",
        "                    total_loss += loss.item() * mask.sum().item()\n",
        "                    total_tokens += mask.sum().item()\n",
        "    \n",
        "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
        "    return avg_loss if not (math.isnan(avg_loss) or math.isinf(avg_loss)) else float('inf')\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, step, val_loss, is_best=False):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'step': step,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'val_loss': val_loss,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'learning_rates': learning_rates,\n",
        "    }\n",
        "    \n",
        "    checkpoint_dir = TRAIN_CONFIG['checkpoint_dir']\n",
        "    torch.save(checkpoint, checkpoint_dir / 'checkpoint_latest.pt')\n",
        "    \n",
        "    if is_best:\n",
        "        torch.save(checkpoint, checkpoint_dir / 'checkpoint_best.pt')\n",
        "        print(f\"  üíæ Saved best model (val_loss: {val_loss:.4f})\")\n",
        "\n",
        "print(\"‚úÖ Training functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Clear GPU cache before training\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# GPU verification with smaller test batch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ Training will use GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ Model parameters on: {next(model.parameters()).device}\")\n",
        "    \n",
        "    # Test forward pass with minimal batch\n",
        "    print(\"\\nüîç Testing GPU computation...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Create a minimal test input instead of using actual batch\n",
        "        test_input = torch.randint(0, vocab_size, (1, 512), device=device)  # Smaller seq len for test\n",
        "        test_mask = torch.ones(1, 512, dtype=torch.long, device=device)\n",
        "        test_output = model(test_input, attention_mask=test_mask)\n",
        "        print(f\"‚úÖ GPU test passed! Output shape: {test_output.shape}\")\n",
        "        \n",
        "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "        print(f\"‚úÖ GPU memory (after test):\")\n",
        "        print(f\"   Allocated: {allocated:.2f} GB\")\n",
        "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
        "    \n",
        "    torch.cuda.empty_cache()  # Clear test cache\n",
        "    model.train()\n",
        "    print(\"\")\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(TRAIN_CONFIG['num_epochs']):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{TRAIN_CONFIG['num_epochs']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    epoch_losses = []\n",
        "    pbar = tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")\n",
        "    \n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        loss = train_step(model, batch, optimizer, criterion, TRAIN_CONFIG['accumulation_steps'], global_step)\n",
        "        \n",
        "        if math.isnan(loss) or math.isinf(loss):\n",
        "            optimizer.zero_grad()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            continue\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            # Periodically clear cache to prevent fragmentation\n",
        "            if batch_idx % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "        \n",
        "        epoch_losses.append(loss)\n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        learning_rates.append(current_lr)\n",
        "        \n",
        "        avg_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else float('nan')\n",
        "        loss_str = f'{loss:.4f}' if not (math.isnan(loss) or math.isinf(loss)) else 'nan'\n",
        "        avg_loss_str = f'{avg_loss:.4f}' if not (math.isnan(avg_loss) or math.isinf(avg_loss)) else 'nan'\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': loss_str,\n",
        "            'lr': f'{current_lr:.2e}',\n",
        "            'avg_loss': avg_loss_str\n",
        "        })\n",
        "        \n",
        "        global_step += 1\n",
        "        \n",
        "        # Validation\n",
        "        if global_step % TRAIN_CONFIG['eval_steps'] == 0:\n",
        "            val_loss = validate(model, val_loader, criterion)\n",
        "            \n",
        "            if math.isnan(val_loss) or math.isinf(val_loss):\n",
        "                val_loss = float('inf')\n",
        "            \n",
        "            val_losses.append(val_loss)\n",
        "            print(f\"\\n  Step {global_step}: Val Loss = {val_loss:.4f}\")\n",
        "            \n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                steps_without_improvement = 0\n",
        "                save_checkpoint(model, optimizer, scheduler, epoch, global_step, val_loss, is_best=True)\n",
        "            else:\n",
        "                steps_without_improvement += 1\n",
        "            \n",
        "            if steps_without_improvement >= TRAIN_CONFIG['patience'] * (len(train_loader) // TRAIN_CONFIG['eval_steps']):\n",
        "                print(f\"\\n‚ö†Ô∏è  Early stopping triggered!\")\n",
        "                break\n",
        "        \n",
        "        # Save checkpoint periodically\n",
        "        if global_step % TRAIN_CONFIG['save_steps'] == 0:\n",
        "            current_val_loss = val_losses[-1] if val_losses else best_val_loss\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, global_step, current_val_loss, is_best=False)\n",
        "    \n",
        "    # Epoch summary\n",
        "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses) if epoch_losses else float('inf')\n",
        "    print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "    print(f\"  Avg Train Loss: {avg_epoch_loss:.4f}\")\n",
        "    if val_losses:\n",
        "        print(f\"  Best Val Loss: {best_val_loss:.4f}\")\n",
        "    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "    \n",
        "    if steps_without_improvement >= TRAIN_CONFIG['patience'] * (len(train_loader) // TRAIN_CONFIG['eval_steps']):\n",
        "        break\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curves\n",
        "axes[0].plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "if val_losses:\n",
        "    val_steps = [i * TRAIN_CONFIG['eval_steps'] for i in range(len(val_losses))]\n",
        "    axes[0].plot(val_steps, val_losses, label='Val Loss', marker='o', markersize=3)\n",
        "axes[0].set_xlabel('Step')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Training and Validation Loss')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Learning rate schedule\n",
        "axes[1].plot(learning_rates, label='Learning Rate', color='green', alpha=0.7)\n",
        "axes[1].set_xlabel('Step')\n",
        "axes[1].set_ylabel('Learning Rate')\n",
        "axes[1].set_title('Learning Rate Schedule')\n",
        "axes[1].set_yscale('log')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(WORKING_DIR / 'training_curves.png', dpi=150)\n",
        "print(f\"‚úÖ Training curves saved to: {WORKING_DIR / 'training_curves.png'}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
